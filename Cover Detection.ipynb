{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E5uN5aFWRkF"
   },
   "source": [
    "# üöÄSetup\n",
    "\n",
    "## Set the runtime type\n",
    "\n",
    "Set the runtime type of this Google Collab to T4 GPU.\n",
    "\n",
    "## Run shell commands\n",
    "\n",
    "You can run shell commands in a cell by using prefix `!`, for example:\n",
    "```\n",
    "!pip install transformers\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd63u9BxXeGN"
   },
   "outputs": [],
   "source": [
    "# Some imports\n",
    "import subprocess, json, os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHOtkUvyWRnG"
   },
   "source": [
    "## Install `insanely-fast-whisper`\n",
    "\n",
    "This is a library to run Whisper model for audio to text transcription.\n",
    "\n",
    "Note that first you need to install `pipx`. Check instructions in `pipx` repository about how to install it in Linux. Then check instructions about how to install `insanely-fast-whisper` in its repo.\n",
    "\n",
    "Check it works well for this URL: https://www.signalogic.com/melp/EngSamples/Orig/male.wav\n",
    "\n",
    "Notes:\n",
    "* The installation is slow, it might take a few minutes.\n",
    "* If `insanely-fast-whisper` executable is not globally available once installed, just run it with its absolute path: `/root/.local/bin/insanely-fast-whisper`. It might be tricky to make it globally available inside this collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x8gsqKKHJKQ"
   },
   "outputs": [],
   "source": [
    "!sudo apt update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "peBx_mtXXflT"
   },
   "outputs": [],
   "source": [
    "!sudo apt install pipx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HPfWF8PXfnr"
   },
   "outputs": [],
   "source": [
    "!pipx install insanely-fast-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C6yTaHoXfqB"
   },
   "outputs": [],
   "source": [
    "!cp /root/.local/bin/insanely-fast-whisper /usr/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_luy8Axr7sY4"
   },
   "outputs": [],
   "source": [
    "!insanely-fast-whisper --file-name https://www.signalogic.com/melp/EngSamples/Orig/male.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whViXS9tWRpo"
   },
   "source": [
    "## Install a python library to download youtube videos\n",
    "\n",
    "\n",
    "There are a few python libraries to download youtube videos, but some of them are not working anymore due to banning issues. For example, `pytube` used to be commonly used for it, but it seems it is not working anymore (see https://www.reddit.com/r/learnpython/comments/1edm1q5/pytube_not_working_please_help/).\n",
    "\n",
    "Find a library that indeed works to download youtube videos, and download some video as audio only (in mp3) to check it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYIQb27FYPfz"
   },
   "outputs": [],
   "source": [
    "!pip install pytubefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6_7iDytYPiR"
   },
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "def download_mp3(url):\n",
    "  yt = YouTube(url, on_progress_callback = on_progress)\n",
    "\n",
    "  # Extract the audio\n",
    "  audio = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "  out_file = audio.download(output_path=\".\")\n",
    "\n",
    "  base, ext = os.path.splitext(out_file)\n",
    "  new_file = base + '.mp3'\n",
    "  os.rename(out_file, new_file)\n",
    "\n",
    "  return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTHjBgfbYPk8"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=yKoF14Mu0CY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_-Sr8RGYPnX"
   },
   "outputs": [],
   "source": [
    "download_mp3(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ag7ixRWWRsH"
   },
   "source": [
    "## Download the lyrics dataset\n",
    "\n",
    "Download this dataset:\n",
    "https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information\n",
    "\n",
    "Using the python code suggested in Kaggle web:\n",
    "```\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"carlosgdcj/genius-song-lyrics-with-language-information\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "```\n",
    "\n",
    "You should find a very large file `song_lyrics.csv`, check it is there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXLd6PMPYWxZ"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"carlosgdcj/genius-song-lyrics-with-language-information\", force_download=True)\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKOLD7krYWzq"
   },
   "outputs": [],
   "source": [
    "!mv /root/.cache/kagglehub/datasets/carlosgdcj/genius-song-lyrics-with-language-information/versions/1/song_lyrics.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CKQHb84WRup"
   },
   "source": [
    "## Install more dependencies\n",
    "\n",
    "Run `!pip install transformers torch faiss-cpu` to install those packages, since they will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsZilYX8Zvms"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfZBa_r3Zs13"
   },
   "source": [
    "\n",
    "\n",
    "# ‚úèÔ∏èDevelopment of solution\n",
    "\n",
    "## Implement `get_lyrics_from_youtube_url(youtube_url)`\n",
    "\n",
    "Implement a function able to extract lyrics as a string from a youtube url using `insanely-fast-whisper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIDjiYvKYZTm"
   },
   "outputs": [],
   "source": [
    "def get_lyrics_from_youtube_url(youtube_url):\n",
    "    try:\n",
    "        # Download the audio\n",
    "        audio_file =  download_mp3(youtube_url)\n",
    "\n",
    "        # Execute Whisper\n",
    "        command = \"insanely-fast-whisper --file-name \" + \"\\\"\" + audio_file + \"\\\"\"\n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "        with open('output.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Clean up the audio file\n",
    "        os.remove(audio_file)\n",
    "        os.remove(\"output.json\")\n",
    "\n",
    "        return data['text']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting lyrics: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEtgTYwMW_bx"
   },
   "source": [
    "## Embeddings extractor\n",
    "\n",
    "Prepare a function able to extract embeddings (for example, BERT), from a given text. In our experience, GPT will provide you with code for this very efficiently.\n",
    "\n",
    "Test it with some string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "iavbXRrLYh5V"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_embeddings(text):\n",
    "  # Load pre-trained tokenizer and model\n",
    "  encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "  embeddings = encoder.encode(text)\n",
    "\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "hUX1LYPeYh7y"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "extract_embeddings(\"Cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye6wvTQZXB9S"
   },
   "source": [
    "## Create a vector database\n",
    "\n",
    "Using `faiss`, create an index with a few embeddings, and use it to search the nearest neighbors from it given a query string.\n",
    "\n",
    "Note that the input to `faiss` must be numpy arrays with proper shape, typically: `(num_items, embedding_dimension)`. For querying only one string, it might require `(1, embedding_dimension)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vDr-yT_YnRw"
   },
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "456RQ2N1KQ8E"
   },
   "outputs": [],
   "source": [
    "# Sample embeddings\n",
    "data = [['Musician in BMAT'], ['Doing stuff in BMAT'], ['Guacamole']]\n",
    "df = pd.DataFrame(data, columns = ['text'])\n",
    "\n",
    "text = df['text'].values\n",
    "vectors = extract_embeddings(text)\n",
    "\n",
    "# Creating index\n",
    "index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "faiss.normalize_L2(vectors)\n",
    "index.add(vectors)\n",
    "\n",
    "# Sample query embedding\n",
    "search_vector = extract_embeddings(\"Testing for BMAT\")\n",
    "testVector = np.array([search_vector])\n",
    "faiss.normalize_L2(testVector)\n",
    "\n",
    "# Search for the nearest neighbor\n",
    "k = index.ntotal\n",
    "distances, indices = index.search(testVector, k)\n",
    "results = pd.DataFrame({'distances': distances[0], 'ann': indices[0]})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7NUTVdNXD38"
   },
   "source": [
    "## Load lyrics database\n",
    "\n",
    "From the databse in `song_lyrics.csv`, we want to extract the top-1000 songs according to views. We will build our vector database with them.\n",
    "\n",
    "Important: This file is huge, and does not fit in RAM. In our case, we did it this way:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "file_path = path + '/song_lyrics.csv'\n",
    "chunksize = 500000\n",
    "top_n = 1000\n",
    "\n",
    "top_views_df = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    chunk_top = chunk.nlargest(top_n, 'views')\n",
    "    top_views_df = pd.concat([top_views_df, chunk_top])\n",
    "    top_views_df = top_views_df.nlargest(top_n, 'views')\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHliUrsJYrA_"
   },
   "outputs": [],
   "source": [
    "# Returns a dataframe of numTop most viewed songs\n",
    "def getDFTop(path, numTop):\n",
    "  chunksize = 500000\n",
    "\n",
    "  top_views_df = pd.DataFrame()\n",
    "\n",
    "  for chunk in pd.read_csv(path, chunksize=chunksize, encoding='utf8', engine='python'):\n",
    "      chunk_top = chunk.nlargest(numTop, 'views')\n",
    "      top_views_df = pd.concat([top_views_df, chunk_top])\n",
    "      top_views_df = top_views_df.nlargest(numTop, 'views')\n",
    "  return top_views_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXeK0spvYrDR"
   },
   "outputs": [],
   "source": [
    "top_views_df = getDFTop('./song_lyrics.csv', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS6IFyAEXFub"
   },
   "source": [
    "## Extract embeddings for lyrics database\n",
    "\n",
    "Extract embeddings for the 1000 lyrics in your database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMJ3_AIZZ4Af"
   },
   "outputs": [],
   "source": [
    "# Extract the lyrics of each song\n",
    "text = top_views_df['lyrics'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYvdnE2uYsWv"
   },
   "outputs": [],
   "source": [
    "# Extract the embeddings of the songs\n",
    "embeddings = extract_embeddings(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4AUtV5NZ42D"
   },
   "source": [
    "\n",
    "## Create a `faiss` index with lyrics\n",
    "\n",
    "Create a `faiss` index with those 1000 lyrics, and test it with some example text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuOA71_BYssO"
   },
   "outputs": [],
   "source": [
    "# Creating index with lyrics\n",
    "def getLyricsIndex(embeddings):\n",
    "  index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "  faiss.normalize_L2(embeddings)\n",
    "  index.add(embeddings)\n",
    "  return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEjy9P4-YvzP"
   },
   "source": [
    "\n",
    "## Implement final function: `get_covers`\n",
    "\n",
    "As described at the beginning of this doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nb07QpceY05m"
   },
   "outputs": [],
   "source": [
    "def get_covers(youtube_url, k):\n",
    "  # Get mp3 from youtube\n",
    "  song = get_lyrics_from_youtube_url(youtube_url)\n",
    "\n",
    "  # Extract embedding from mp3\n",
    "  embedding = extract_embeddings(song)\n",
    "  embedding = np.array([embedding])\n",
    "  faiss.normalize_L2(embedding)\n",
    "\n",
    "  # Extract the lyrics of each song\n",
    "  lyrics = top_views_df['lyrics'].values\n",
    "\n",
    "  # Extract the embeddings of the songs\n",
    "  embeddings = extract_embeddings(lyrics)\n",
    "\n",
    "  # Create the index\n",
    "  index = getLyricsIndex(embeddings)\n",
    "\n",
    "  # Search for the nearest neighbor\n",
    "  distances, indices = index.search(embedding, k)\n",
    "  results = pd.DataFrame({'distances': distances[0], 'ann': indices[0]})\n",
    "\n",
    "  results['title'] = top_views_df['title'].iloc[results['ann'].values].values\n",
    "  results['artist'] = top_views_df['artist'].iloc[results['ann'].values].values\n",
    "\n",
    "  # Return similarity as percentage: sim = 100 * (1-D)\n",
    "  results['score'] = results['distances'].apply(lambda row: round(100 * (1 - row), 1))\n",
    "  results.drop(['distances', 'ann'], axis=1, inplace=True)\n",
    "\n",
    "  # Return as: {\"title\": \"Title 1\", \"artist\": \"Artist 1\", \"score\": 95.0}\n",
    "  return results.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKyEOwCAXI7s"
   },
   "source": [
    "## üìäEvaluation of your solution\n",
    "\n",
    "Let's evaluate the system with 8 youtube videos:\n",
    "\n",
    "* https://www.youtube.com/watch?v=BDC8Jr-gp_4\n",
    "* https://www.youtube.com/watch?v=W_97b97G5ds\n",
    "* https://www.youtube.com/watch?v=L53MZzuE0QY\n",
    "* https://www.youtube.com/watch?v=9vmrPrYJPqI\n",
    "* https://www.youtube.com/watch?v=R6ATpAr7rQU\n",
    "* https://www.youtube.com/watch?v=RmtP8X4ZErs\n",
    "* https://www.youtube.com/watch?v=DfMnRP0pk3A\n",
    "* https://www.youtube.com/watch?v=1BVP72VrGQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ5pk5McoVoK"
   },
   "outputs": [],
   "source": [
    "# Create the dataframe if not created\n",
    "top_views_df = getDFTop('./song_lyrics.csv', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbQIKQ_jY2qm"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "k = 1\n",
    "get_covers(\"https://www.youtube.com/watch?v=BDC8Jr-gp_4\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWVi4joxY2vi"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=W_97b97G5ds\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAvGdBN4Y2yD"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=L53MZzuE0QY\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEyl_uXTY20X"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=9vmrPrYJPqI\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "oCW3g3rBuVwU"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=R6ATpAr7rQU\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjGjopzrZ40o"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=RmtP8X4ZErs\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGfGlFs1Z4rx"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=DfMnRP0pk3A\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZjDo89UZ4fz"
   },
   "outputs": [],
   "source": [
    "get_covers(\"https://www.youtube.com/watch?v=1BVP72VrGQs\", k)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8E5uN5aFWRkF",
    "whViXS9tWRpo",
    "0Ag7ixRWWRsH",
    "5CKQHb84WRup",
    "Ye6wvTQZXB9S",
    "s7NUTVdNXD38",
    "YS6IFyAEXFub",
    "f4AUtV5NZ42D"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
